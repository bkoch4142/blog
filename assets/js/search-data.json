{
  
    
        "post0": {
            "title": "LSTM - Intuition, Theory, Implementation",
            "content": ". Note: This post builds on my previous explanation of RNNs. The dataset used in this post is the IMDB dataset of 50,000 movie reviews, used for sentiment classification. . RNN vs LSTM . LSTM, shorthand for Long-Short-Term-Memory, is a recurrent architecture for processing sequences just as the vanilla RNN. Compared to LSTM, the vanilla RNN is not used in practice as much, as it has some notable limitations that the LSTM architecture tries to address. These include: . Exploding/Vanishing Activations | Memory Limitations | . Exploding/Vanishing Activations . The problem of exploding/vanishing gradients occurs due to the way the vanilla RNN backpropagates gradients. If you consider the unrolled representation of an RNN and its backpropagation mechanism you will quickly notice that for long sequences there is a lot of repeated multiplication done in order for gradients to arrive from the last layer to the initial one. This is the exact reason why we might experience exploding/vanishing gradients. Let&#39;s explore this problem in more detail by simulating repeated multiplication in a backpropagation. . Let say we have a sequence of length 50. We will multiply matrices initialized with random numbers from a normal distribution. . x=torch.randn(50,50) for i in range(50): x= x @ torch.randn(50,50) x . tensor([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]]) . This is the occurence of exploding gradients. Basically, our numbers got so large that it caused numerical overflow and produced NaNs. What if we try to mitigate this by decreasing our matrices by a factor of 0.01? . x=torch.randn(50,50) for i in range(50): x= x @ torch.randn(50,50)*0.01 x . tensor([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) . Now, this produced vanishing gradients or numerical underflow. The numbers became so small that our computer just represents them with zeros. If we would have a weight matrix like this in any part of our neural network it would break the models learning capability, so that is why this is a delicate but important problem to solve. . Solutions to this include: . Identity initialization of weights paired with ReLU activation | Gradient clipping | Skip connections | Model alteration | . LSTM is an instance of a model approach to this problem. It mitigates the vanishing/exploding gradients problem by introducing the concept of gates which change the way gradients flow. . Memory Limitations . If you would experiment with RNNs, e.g. for next-word-prediction (language model), you would notice that they have a hard time with long-term memory needed in sentences such as this: . &quot;I grew up in France and had a wonderful childhood, therefore I speak fluent French&quot; -The language model would need to retain the information of &quot;France&quot; until its useful for predicting the word &quot;French&quot;. . LSTMs do a much better job with long-term memory because of several reasons: . An additional state called the cell state which enables accumulation of information over a long duration. | The concept of memory management depending on the current timestep&#39;s input and hidden state. | . Cell State . LSTMs have an additional state called the cell state which is passed along with the hidden state to each cell. But the cell state has no linear layers through which it passes, therefore enabling easier information flow over a longer duration. This is what enables the long-term memory of an LSTM. Cell state is only influenced by element-wise operations controlled by gates which we will observe soon. . Memory Management . The term &quot;gated RNN&quot; comes from the fact that the cell state is gated (protected) by so-called gates. These gates are linear layers responsible for managing the cell state by extracting relevant information from the current timestep input and hidden state. The idea is that at each timestep the cell state information we don&#39;t need anymore should be forgotten and new valuable information should be stored. Since these gates are layers we delegate this mechanism for the neural network to learn itself without us having to manage it manually. . Architecture . This is what the LSTM Cell looks like: . . There are four gates and each of them is fed the stacked tensor of the current timestep&#39;s hidden state and input. In summary, what each gate does for a sequence at a particular timestep is the following: . Forget gate: what information to keep and what to forget in the long memory | Input gate: what information needs to be updated in the long memory | Cell gate: how the information will be updated in the long memory | Output gate: what part of the long memory is relevant for the short memory | . Forget Gate . Forget gate decides what to forget/eliminate from the cell state. It is followed by a sigmoid function, mapping values to the (0, 1) range. We then multiply this output element-wise with the cell state. If the scalar at some position of the sigmoid output is closer to 0 it will result in the elimination of the value at the same position in the cell state. The opposite is true for values close to 1. . Input Gate and Cell Gate . Input and Cell gate together decide what to update/store in the cell state. Input gate decides what needs to be updated and to what degree, dictated by the sigmoid. Cell gate decides what are the updated values for positions chosen by the input gate. Cell gate is followed by the tanh function mapping values to (-1, 1) range. The output of the input and cell gate is multiplied element-wise and then added element wise to the cell state. . Output Gate . Output gate decides which information from the cell state is relevant for the next hidden state. Then this is fed to the sigmoid and the output is multiplied element-wise with the tanh of the updated cell state . Implementation . Below is a PyTorch implemenation of the LSTM Cell we described. . class LSTMCell(nn.Module): def __init__(self,n_input,n_hidden): super(LSTMCell,self).__init__() self.forget_gate=nn.Linear(n_input+n_hidden, n_hidden) self.input_gate=nn.Linear(n_input+n_hidden, n_hidden) self.cell_gate=nn.Linear(n_input+n_hidden, n_hidden) self.output_gate=nn.Linear(n_input+n_hidden, n_hidden) def forward(self, x, state): h,c=state forget_gate_out=torch.sigmoid(self.forget_gate(x)) input_gate_out=torch.sigmoid(self.input_gate(x)) cell_gate_out=torch.sigmoid(self.cell_gate(x)) output_gate_out=torch.sigmoid(self.output_gate(x)) c=c*forget_gate_out c=c+(input_gate_out*cell_gate_out) out=output_gate_out*torch.tanh(c) return out,(h,c) . . Note: This particular implementation is not efficient, its only a toy example. For an efficient implementation merge the 4 matrix multiplications into 1. . Hopefully, all parts of an LSTM are now clear. For an even better understanding continue to see it applied to sentiment analysis. . Performance . We will be testing LSTM&#39;s performance on the IMDB dataset for sentiment analysis. . Preparing the dataset . data=pd.read_csv(&#39;./imdb.csv&#39;) data=data.sample(frac=1) print(data.head()) . review sentiment 619 The quote above just about says it all for &#34;Sl... negative 33422 normally,i would say i loved this movie.not fo... negative 28228 When i heard about this movie it was supposed ... positive 28504 (You&#39;ll know what I mean after you&#39;ve seen Red... positive 8085 I enjoyed Still Crazy more than any film I hav... positive . train=data[:25000] xtrain=train[&#39;review&#39;].values.tolist() ytrain=train[&#39;sentiment&#39;].values val=data[25000:] xval=val[&#39;review&#39;].values.tolist() yval=val[&#39;sentiment&#39;].values # Preprocessing (Tokenization and padding) tokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=3000) tokenizer.fit_on_texts(xtrain) xtrain_pro=tokenizer.texts_to_sequences(xtrain) xtrain_pro=tf.keras.preprocessing.sequence.pad_sequences(xtrain_pro, maxlen=128) xval_pro=tokenizer.texts_to_sequences(xval) xval_pro=tf.keras.preprocessing.sequence.pad_sequences(xval_pro, maxlen=128) ytrain=[1 if y==&#39;positive&#39; else 0 for y in ytrain] yval=[1 if y==&#39;positive&#39; else 0 for y in yval] . Now we create our dataset class and our datasets . class IMDBDataset: def __init__(self,reviews,targets): self.reviews=reviews self.targets=targets def __len__(self): return len(self.reviews) def __getitem__(self,idx): review=self.reviews[idx,:] target=self.targets[idx] review=torch.tensor(review,dtype=torch.long) target=torch.tensor(target,dtype=torch.float) item=(review, target) return item train_ds=IMDBDataset(xtrain_pro, ytrain) val_ds=IMDBDataset(xval_pro, yval) . Lastly, we create our dataloaders . train_dl=torch.utils.data.DataLoader( train_ds, batch_size=128, ) val_dl=torch.utils.data.DataLoader( val_ds, batch_size=128, ) . Constructing the SentimentClassifier Model with our LSTM . In order to apply the LSTMCell we package it in the LSTM class which applies it to a sequence. . class LSTM(nn.Module): def __init__(self,n_input,n_hidden): super(LSTM,self).__init__() self.n_input=n_input self.n_hidden=n_hidden self.LSTMCell=LSTMCell(n_input,n_hidden) def forward(self,input,state=None): # Input dims are (batch_size, seq_length, timestep_features) sequence_length=input.size()[1] # Initialize hidden state to zeros if not provided if state==None: state=(torch.zeros((input.size()[0],self.n_hidden),device=input.device), torch.zeros((input.size()[0],self.n_hidden),device=input.device)) h,c=state outs=torch.tensor([],device=input.device) for i in range(sequence_length): x_timestep_features=torch.squeeze(input[:,i,:],dim=1) x_timestep_features=torch.cat([h, x_timestep_features], dim=1) out, (h,c)=self.LSTMCell(x_timestep_features, (h,c)) out=torch.unsqueeze(out,dim=1) outs=torch.cat((outs,out), dim=1) return outs . Finally we package our LSTM into a SentimentClassifier class which uses an additional embedding layer and linear layer. . class SentimentClassifier(nn.Module): def __init__(self,vocab_sz,n_hidden): super(SentimentClassifier,self).__init__() self.embedding=nn.Embedding(vocab_sz,n_hidden) self.rnn=LSTM(n_hidden,n_hidden) self.linear=nn.Linear(n_hidden*2,1) def forward(self, x): x=self.embedding(x) x =self.rnn(x) # Using the avg and max pool of all RNN outputs avg_pool=torch.mean(x, dim=1) max_pool, _ =torch.max(x,1) # We concatenate them (hidden size before the linear layer is multiplied by 2) out=torch.cat((avg_pool, max_pool), dim=1) out=self.linear(out) return torch.squeeze(out, dim=1) . Constructing the training loop . device=&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; device . &#39;cuda&#39; . model=SentimentClassifier(3000,100) model=model.to(device) loss_func=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(model.parameters(), lr=1e-3) . epochs=5 for epoch in range(epochs): # Training ys=[] y_preds=[] for x,y in train_dl: x=x.to(device) y=y.to(device) y_pred=model(x) loss=loss_func(y_pred,y) optimizer.zero_grad() loss.backward() optimizer.step() ys.extend(y.detach().cpu().numpy().tolist()) y_preds.extend(y_pred.detach().cpu().numpy().tolist()) # Measuring Training accuracy y_preds_final=sigmoid(np.array(y_preds))&gt;0.5 accuracy=accuracy_score(y_preds_final,ys) print(f&quot;Epoch: {epoch} | Training accuracy: {accuracy:.2f}&quot;) # Validation ys=[] y_preds=[] with torch.no_grad(): for x,y in val_dl: x=x.to(device) y=y.to(device) y_pred=model(x) ys.extend(y.detach().cpu().numpy().tolist()) y_preds.extend(y_pred.detach().cpu().numpy().tolist()) # Measuring Validation accuracy y_preds_final=sigmoid(np.array(y_preds))&gt;0.5 accuracy=accuracy_score(y_preds_final,ys) print(f&quot;Epoch: {epoch} | Validation accuracy: {accuracy:.2f}&quot;) print(&quot;-&quot;*40) . Epoch: 0 | Training accuracy: 0.70 Epoch: 0 | Validation accuracy: 0.77 - Epoch: 1 | Training accuracy: 0.81 Epoch: 1 | Validation accuracy: 0.83 - Epoch: 2 | Training accuracy: 0.85 Epoch: 2 | Validation accuracy: 0.85 - Epoch: 3 | Training accuracy: 0.88 Epoch: 3 | Validation accuracy: 0.86 - Epoch: 4 | Training accuracy: 0.89 Epoch: 4 | Validation accuracy: 0.87 - . For the end we can test our model on arbitrary input. . def predict(model,text): x=tokenizer.texts_to_sequences(text) x=tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=128) x=torch.tensor(x,device=&#39;cuda&#39;) model.eval() with torch.no_grad(): logit=model(x) prob=sigmoid(logit.cpu().numpy()) print(f&quot;Output: {str(prob)} | &quot;, end=&#39;&#39;) if prob&gt;=0.5: print(f&#39;Sentiment: positive&#39;) else: print(f&#39;Sentiment: negative&#39;) . . text=[&#39;This guy made this blog about LSTMs and provided the implementation without explaining each line of the code!&#39;] predict(model,text) . Output: [0.13759616] | Sentiment: negative . Thanks for Reading! .",
            "url": "https://bkoch4142.github.io/blog/nlp/2020/11/08/LSTM.html",
            "relUrl": "/nlp/2020/11/08/LSTM.html",
            "date": " • Nov 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "RNN's - Intuition, Theory, Implementation",
            "content": "Problem Description . “Machines take me by surprise with great frequency.” ― Alan Turing . Before we describe a solution let&#39;s describe the problem which gave rise to RNN&#39;s. The problem is sequences, i.e. how do we train models that understand and make use of sequential data. An example of sequential data is simply a sentence, text. The most essential difference from non-sequential data is that by changing the order inside a sequence we might change the whole meaning. . How is sequential data useful? Research shows that powerful deep learning models trained on sequential data like text can implicitly learn a lot about our language. This ranges from simpler models being able to determine the sentiment of text (e.g. is a movie review &quot;positive&quot; or &quot;negative&quot;) to a model being able to write a whole paragraph on a subject itself while being very convincing. . Language is especially interesting because it offers such a broad portal for expression. This is why Alan Turing used Language/Conversation as a measuring stick for Intelligence in its paper “Computing Machinery And Intelligence” and his famous Turing Test. . Instead of thinking abstractly about sequential data and the problem of sequential data let&#39;s take a look at a real problem, the problem of predicting the next word in a sequence. We will be using a dataset called HUMAN_NUMBERS. The dataset is simply a letter representation of the first 10,000 numbers written in English. . !pip install fastai --upgrade . . from fastai.text.all import * path=untar_data(URLs.HUMAN_NUMBERS) lines=L() with open(path/&#39;train.txt&#39;) as f: lines+= L(*f.readlines()) with open(path/&#39;valid.txt&#39;) as f: lines+= L(*f.readlines()) text=&#39; . &#39;.join([l.strip() for l in lines]); text[:100] . &#39;one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo&#39; . As with every data a machine learning model consumes, it should be expressed in the language of machines - numbers. How do we turn text to numbers? We do this in two steps: . tokenization | numericalization | . Tokenization is just splitting a sequence into its subparts (in this case those will be words). This way we can look at each of its building blocks and how diverse are they. In our case we have . tokens=text.split(&#39; &#39;); tokens[:10] . [&#39;one&#39;, &#39;.&#39;, &#39;two&#39;, &#39;.&#39;, &#39;three&#39;, &#39;.&#39;, &#39;four&#39;, &#39;.&#39;, &#39;five&#39;, &#39;.&#39;] . After we tokenize the whole text we basically have a vocabulary of tokens. Our whole text can actually be represented with the 30 tokens listed below. . vocab=L(*tokens).unique(); print(vocab) . [&#39;one&#39;, &#39;.&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;five&#39;, &#39;six&#39;, &#39;seven&#39;, &#39;eight&#39;, &#39;nine&#39;, &#39;ten&#39;, &#39;eleven&#39;, &#39;twelve&#39;, &#39;thirteen&#39;, &#39;fourteen&#39;, &#39;fifteen&#39;, &#39;sixteen&#39;, &#39;seventeen&#39;, &#39;eighteen&#39;, &#39;nineteen&#39;, &#39;twenty&#39;, &#39;thirty&#39;, &#39;forty&#39;, &#39;fifty&#39;, &#39;sixty&#39;, &#39;seventy&#39;, &#39;eighty&#39;, &#39;ninety&#39;, &#39;hundred&#39;, &#39;thousand&#39;] . Now we do numericalization - we assign a number to each token and that&#39;s how we will represent them to our model. We will define the mapping with a dictionary . word2idx={w:i for i,w in enumerate(vocab)} print(word2idx) . {&#39;one&#39;: 0, &#39;.&#39;: 1, &#39;two&#39;: 2, &#39;three&#39;: 3, &#39;four&#39;: 4, &#39;five&#39;: 5, &#39;six&#39;: 6, &#39;seven&#39;: 7, &#39;eight&#39;: 8, &#39;nine&#39;: 9, &#39;ten&#39;: 10, &#39;eleven&#39;: 11, &#39;twelve&#39;: 12, &#39;thirteen&#39;: 13, &#39;fourteen&#39;: 14, &#39;fifteen&#39;: 15, &#39;sixteen&#39;: 16, &#39;seventeen&#39;: 17, &#39;eighteen&#39;: 18, &#39;nineteen&#39;: 19, &#39;twenty&#39;: 20, &#39;thirty&#39;: 21, &#39;forty&#39;: 22, &#39;fifty&#39;: 23, &#39;sixty&#39;: 24, &#39;seventy&#39;: 25, &#39;eighty&#39;: 26, &#39;ninety&#39;: 27, &#39;hundred&#39;: 28, &#39;thousand&#39;: 29} . Our whole text is now a list of numbers: . nums=L(word2idx[i] for i in tokens); nums . (#63095) [0,1,2,1,3,1,4,1,5,1...] . . Note: These are only the most basic forms of tokenization and numericalization. . For a start we will want our model to predict the next word in a sequence having seen the three words before it. Ponder for a moment what this means. A model doesn&#39;t understand English, nor does it understand how counting in English works, it doesn&#39;t even know what a number is. This is somewhat equivalent to someone giving you a task to learn to count up to 10,000 in a foreign language without you knowing what counting means. We will create a supervised dataset for this by using sub-sequences of three words as our independent variables and the next word after each sub-sequence as the dependant variable. . seqs=L( (tensor(nums[i:i+3]),tensor(nums[i+3])) for i in range(0,len(nums)-4,3)); seqs . (#21031) [(tensor([0, 1, 2]), tensor(1)),(tensor([1, 3, 1]), tensor(4)),(tensor([4, 1, 5]), tensor(1)),(tensor([1, 6, 1]), tensor(7)),(tensor([7, 1, 8]), tensor(1)),(tensor([1, 9, 1]), tensor(10)),(tensor([10, 1, 11]), tensor(1)),(tensor([ 1, 12, 1]), tensor(13)),(tensor([13, 1, 14]), tensor(1)),(tensor([ 1, 15, 1]), tensor(16))...] . cut=int(len(seqs)*0.8) dls=DataLoaders.from_dsets(seqs[:cut],seqs[cut:],bs=64,shuffle=False) . What Is Wrong With FFNN&#39;s? . “We can only see a short distance ahead, but we can see plenty there that needs to be done.” ― Alan Turing . You might be asking yourself; okay... why can&#39;t we just use FFNN&#39;s for this problem? We can! And let&#39;s imagine we do! Take a look at the picture of a FFNN below. It has three input neurons as we want and it has one output neuron just like we would want, now what is the problem with that? . . No Concept Of Time Resembled In The Architecture . Look at the connections between the neurons, everything is fed at once, observed at once, and connected to everything. Why is this bad? Wouldn&#39;t we weaken the network if we restricted it so that the neural network looks at inputs sequentially before deciding the output? We would, but that is exactly what we would want - if we would restrict the neural network like this we would eliminate the need for a neural network to LEARN the concept of time as it would already be there. . Tip: For an even better intuition imagine if you would watch a movie in a way so that all images appear on a huge screen at once rather than being displayed one at a time. It would be much harder to understand what is going on even though you can see more at once because you need to manually introduce the concept of time by forcefully looking at one image at a time. FFNN&#8217;s having to learn this concept of time introduces an additional overhead that makes them inappropriate for sequences. . Inputs Can&#39;t Be Of Arbitrary Lengths . FFNN&#39;s can only receive inputs of constant dimensionality. Sentences can be different lengths, we would like our architecture to be able to process any sequence regardless of length and map them to the same feature space. A text classifier wouldn&#39;t be of much use if it could only classify sentences of exactly, let&#39;s say, 20 words. . In our example of human numbers we do have all the independent variables as vectors of length 3, but we will change that later. . No Memory Of Past Samples/Batches . In text the preceding sentence may carry information valuable for the interpretation of the next one. Even if our FFNN could handle inputs of arbitrary lengths there would be no information carried from the past sentence to the current one. It would be useful if our NN would have some sort of memory to be able to handle this. . RNN . “A computer would deserve to be called intelligent if it could deceive a human into believing that it was human” ― Alan Turing . Let&#39;s first describe the RNN architecture: . Important: RNN&#8217;s are neural networks where we apply the same layer multiple times. . That&#39;s basically it, there is nothing magical about it, yet this solves all of our mentioned problems to a degree. Let&#39;s apply this technique to our problem. We will reuse a single linear layer for each of our three independent variables. . We will also use an embedding layer before the linear layer and therefore map our integer representation of our words to a vector representation with learnable parameters. I won&#39;t go into detail about embeddings in this blog, I plan to release a post about embeddings so check my blog. Just think of embeddings as giving our model freedom to save some info about each of our words, currently represented by integers, by defining a mapping of each of them to a vector which is fed to the linear layer instead of the integer. . Our model would now look like this: . . With the use of hashtags I pointed out that we are using the exact same linear layer #1 multiple times in our model. You can either think of these linear layers #1 as one and the same (which is how it&#39;s implemented in code) or you can think of them as linear layers having the same parameters, in other words, sharing parameters. This way at the beginning the linear layer #1 only uses the first word&#39;s embedding to produce an output, then it feeds its output back to itself (or to the next layer being identical to itself) while adding to it the second word&#39;s embedding, then it takes that output, combines it with the third word&#39;s embedding and feeds it to itself again. This constant &#39;reuse&#39; of the same layer during training forces the layer to learn handling any position in a sequence rather than being biased to one or the other. . &quot;The way that one word impacts the activations from previous words should not change depending on the position of a word. In other words, activation values will change as data moves through the layers, but the layer weights themselves will not change from layer to layer. So, a layer does not learn one sequence position; it must learn to handle all positions.&quot; ― Deep Learning for Coders with fastai &amp; PyTorch 1 . The intermediary outputs that are actually fed back to the same layer are called hidden states. The linear layer #2 is the final layer that produces a vector from the last hidden state. This vector&#39;s length is same as our vocab size and its each entry represents the confidence of each word being the output. . Alright, now how did this help? It did the following: . It introduced a concept of time by the model being fed a word per time step $t$. Each word is therefore interpreted in the context of the words preceding it. | It enabled the handling of arbitrary lengths as we could repeat our linear layer as much as we want. | It introduced a concept of memory by feeding our layer the output of the previous time step. | . Implementation: . class RNN1(Module): def __init__(self,vocab_sz,n_hidden): self.i_h=nn.Embedding(vocab_sz,n_hidden) self.h_h=nn.Linear(n_hidden,n_hidden) self.h_o=nn.Linear(n_hidden,vocab_sz) def forward(self,x): h=self.i_h(x[:,0]) h=self.h_h(h) h=F.relu(h) h=h+self.i_h(x[:,1]) h=self.h_h(h) h=F.relu(h) h=h+self.i_h(x[:,2]) h=self.h_h(h) h=F.relu(h) h=self.h_o(h) return h . bs=64 learn=Learner(dls,RNN1(len(vocab),bs),loss_func=F.cross_entropy,metrics=accuracy) learn.fit_one_cycle(4,1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.833544 | 1.998925 | 0.394818 | 00:01 | . 1 | 1.409288 | 1.783905 | 0.464702 | 00:01 | . 2 | 1.430346 | 1.669921 | 0.487045 | 00:01 | . 3 | 1.393818 | 1.724913 | 0.424055 | 00:01 | . . Note: The most common token in our dataset is the word &#8217;thousand&#8217; which if we were to always predict as output would achieve 15% accuracy. This means our model is quite better than a Naive model that would only predict the most frequent token every time, increasing its odds of guessing the correct word. . Now that we understand how RNN&#39;s work let&#39;s look at their most common ilustration: . . We can simplify our code to resemble this: . class RNN2(Module): def __init__(self,vocab_sz,n_hidden): self.i_h=nn.Embedding(vocab_sz,n_hidden) self.h_h=nn.Linear(n_hidden,n_hidden) self.h_o=nn.Linear(n_hidden,vocab_sz) def forward(self,x): h=0 for i in range(3): h=h+self.i_h(x[:,i]) h=self.h_h(h) h=F.relu(h) return self.h_o(h) . learn=Learner(dls,RNN2(len(vocab),bs),loss_func=F.cross_entropy,metrics=accuracy) learn.fit_one_cycle(4,1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.765434 | 1.949799 | 0.465652 | 00:01 | . 1 | 1.372692 | 1.805319 | 0.467079 | 00:01 | . 2 | 1.391457 | 1.660083 | 0.489660 | 00:01 | . 3 | 1.365470 | 1.680930 | 0.463989 | 00:01 | . Now you understand how an RNN works. If you would like to know how to improve over this basic version let&#39;s continue. . Improving The RNN . Maintaining The State Of An RNN . “We like to believe that Man is in some subtle way superior to the rest of creation. It is best if he can be shown to be necessarily superior, for then there is no danger of him losing his commanding position.” ― Alan Turing . Notice how our model&#39;s hidden state is initialized to 0 for every sample we process. It would be better if we would carry the information from the previous sample over to the next one. We can do that by moving the hidden state initialization to the constructor and maintaining it over samples. We just have to be careful to detach our computation graph so we don&#39;t backpropagate over 10,000 layers. We also need to change how we feed the data to the model, every sample has to continue on the next one. . class RNN3(Module): def __init__(self,vocab_sz,n_hidden): self.i_h=nn.Embedding(vocab_sz,n_hidden) self.h_h=nn.Linear(n_hidden,n_hidden) self.h_o=nn.Linear(n_hidden,vocab_sz) self.h=0 def forward(self,x): for i in range(3): self.h=self.h+self.i_h(x[:,i]) self.h=self.h_h(self.h) self.h=F.relu(self.h) out=self.h_o(self.h) self.h=self.h.detach() return out def reset(self): self.h=0 . def get_batches(ds,bs): batch_count=len(ds) // bs batches=L() for i in range(batch_count): batches += L( ds[i+batch_count*j] for j in range(bs)) return batches cut=int(len(seqs)*0.8) dls=DataLoaders.from_dsets( get_batches(seqs[:cut], bs), get_batches(seqs[cut:],bs), bs=bs, drop_last=True, shuffle=False ) . learn=Learner(dls,RNN3(len(vocab),bs),loss_func=F.cross_entropy,metrics=accuracy) learn.fit_one_cycle(10,1e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.396722 | 2.262319 | 0.462500 | 00:01 | . 1 | 1.450032 | 1.849707 | 0.448558 | 00:01 | . 2 | 1.256673 | 1.803644 | 0.414183 | 00:01 | . 3 | 1.084279 | 1.692557 | 0.472356 | 00:01 | . 4 | 0.997137 | 1.557435 | 0.510577 | 00:01 | . 5 | 0.944075 | 1.430444 | 0.558654 | 00:01 | . 6 | 0.916451 | 1.541630 | 0.558413 | 00:01 | . 7 | 0.865630 | 1.551041 | 0.565865 | 00:01 | . 8 | 0.843312 | 1.601054 | 0.564183 | 00:01 | . 9 | 0.835393 | 1.549224 | 0.575721 | 00:01 | . Creating More Signal . “I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.” ― Alan Turing . Currently we only predict a single output word for three input words. This limits the amount of signal we feed back to the network with backpropagation. If we would predict the next word after every word then the backpropagation could provide much more information for correct weight adaptation. Instead of feeding chunks of three words we will now use sequence length of 16. . sl=16 seqs=L( (tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl)) cut=int(len(seqs)*0.8) dls=DataLoaders.from_dsets( get_batches(seqs[:cut], bs), get_batches(seqs[cut:],bs), bs=bs, drop_last=True, shuffle=False ) . class RNN4(Module): def __init__(self, vocab_sz, n_hidden): self.i_h=nn.Embedding(vocab_sz, n_hidden) self.h_h=nn.Linear(n_hidden,n_hidden) self.h_o=nn.Linear(n_hidden,vocab_sz) self.h=0 def forward(self, x): outs=[] for i in range(sl): self.h=self.h+self.i_h(x[:,i]) self.h=self.h_h(self.h) self.h=F.relu(self.h) outs.append(self.h_o(self.h)) self.h=self.h.detach() return torch.stack(outs,dim=1) def reset(self): self.h=0 . def loss_func(inp,targ): return F.cross_entropy(inp.view(-1,len(vocab)), targ.view(-1)) . learn=Learner(dls, RNN4(len(vocab),bs), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15,3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.282630 | 3.128972 | 0.226807 | 00:00 | . 1 | 2.394483 | 2.054682 | 0.459066 | 00:00 | . 2 | 1.771564 | 1.961018 | 0.393311 | 00:01 | . 3 | 1.449017 | 1.886496 | 0.489990 | 00:00 | . 4 | 1.228446 | 1.729708 | 0.499268 | 00:00 | . 5 | 1.065741 | 1.494743 | 0.559408 | 00:00 | . 6 | 0.927685 | 1.429495 | 0.598145 | 00:00 | . 7 | 0.802961 | 1.388741 | 0.597087 | 00:00 | . 8 | 0.716347 | 1.358838 | 0.610189 | 00:00 | . 9 | 0.652654 | 1.409269 | 0.621501 | 00:00 | . 10 | 0.608551 | 1.405770 | 0.663411 | 00:00 | . 11 | 0.563584 | 1.389074 | 0.661621 | 00:00 | . 12 | 0.535806 | 1.442170 | 0.663818 | 00:00 | . 13 | 0.513522 | 1.438936 | 0.654460 | 00:00 | . 14 | 0.500440 | 1.416253 | 0.658773 | 00:00 | . Going further . Next topics that improve upon the mentioned concepts: . LSTM | GRU | Seq2Seq | Attention | Transformers | . Stay tuned! . 1. Book by fastai: Deep Learning for Coders with fastai and PyTorch↩ . 2. Course by fastai: Practical Deep Learning for Coders↩ .",
            "url": "https://bkoch4142.github.io/blog/jupyter/nlp/2020/10/22/RNN-Intuition-Theory-Implementation.html",
            "relUrl": "/jupyter/nlp/2020/10/22/RNN-Intuition-Theory-Implementation.html",
            "date": " • Oct 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Constructing an artificial neural network",
            "content": "Introducing the Neural Network . Before going through this notebook, if you are a beginner, it can be useful to storm through this video by 3Blue1Brown, which will offer some visual intuition before you attack the text and code below: . The word neural in &quot;neural network&quot; comes from neurobiology from where the neural network architecture was loosely inspired, and the word &#39;network&#39; is used because it encompasses multiple connected functions inside it resembling a network. In the process of explaining and constructing the neural network, we will be solving a supervised task. A supervised task is a task where there is a dataset of paired inputs and outputs: . $$ left { left(x_{i}, y_{i} right) right }_{i=1}^{N}$$ . With the use of neural networks, these paired inputs and outputs are assumed to have functional dependence which we don&#39;t know and are trying to approximate with it. . For example, later in this blog, we will try to teach/train a neural network to recognize does an image (input) represent a cat or not (output), therefore we assume that there is functional dependence between the pixels in the image and the output classes &#39;cat&#39; and &#39;not-cat&#39;. . What our neural network will then help us do is find that function or its close approximate. Therefore we come to an important conclusion: . Important: Neural networks are function approximation machines. . A neural network is said to be able to approximate any function at all. More precisely, no matter what the function f is, there is guaranteed to be a neural network so that for every possible input x the neural network outputs f(x) or some close approximate. . Tip: This theorem is called &quot;The Universal Approximation Theorem&quot; . The real challenge is finding such a network and specializing it for our task. . Neural networks can approximate functions by being itself a chain of functions with adjustable parameters. . $$y=f_{NN}(x)=f_{L} left( ldots f_{2} left(f_{1}(x) right) right)$$ . Individual functions in this equation are called layers, each representing a vector-to-vector operation. To better understand the neural network building blocks we can think of layers as consisting out of multiple simple vector-to-scalar computational units called neurons. These neurons are connected to each of the outputs of the preceding layer and calculate a weighted sum and apply an activation function on those inputs which produces an output. . . Where: . $x_i$ is a real number called input | $ omega_i$ is a real number called weight | $ varphi$ is a function applied to the weighted sum called an activation function | $b$ is a real number called bias | $y$ is a real number called the output of a neuron | . $ omega$ and $b$ are called the parameters of the neural network. . Mathematical calculations of a whole layer of neurons can be represented with the following formula: . $$f_{l}(z)= varphi_{l} left( mathbf{W}_{l} mathbf{y}_{l-1}+ mathbf{b}_{l} right)$$ . Where: . $ mathbf{W}_l$ is a matrix of weights for the current layer | $ mathbf{y}_{l-1}$ is a vector of inputs to this layer | $ mathbf{b}_l$ is a vector of biases | . By connecting multiple layers, we get the feedforward neural network. An example of a neural network is presented below. The first layer is called the input layer and the last one is called the output layer. Intermediary layers are called hidden layers. . . Building a neural network includes selecting its parameters as well as hyperparameters. . Selecting hyperparameters means deciding its architecture in terms of the number of layers, number of neurons in each layer, activation functions, etc. These manage the possibility of a neural network to approximate a function and are usually selected manually. . Parameters of a neural network are on the other hand represented by weight matrices $ mathbf{W}_l $ and bias vectors $ mathbf{b}_l$ for each layer. Parameters control what a neural network does and are exactly what the process referred to as neural network training is trying to optimize. . Important: The ability of a neural network to represent a variety of functions is dependant on its hyperparameters while parameters control how exactly the approximation is done. . For each neural network a cost function needs to be defined. A cost function takes in the output of the neural network and evaluates its performance on the dataset. After the evaluation of a neural network its parameters can be adjusted to produce a better output with the algorithm called backpropagation. . Constructing a Neural Network . Parameter Initialization . First, we have to decide the number of layers and their dimensions. We will allow this to be adjustable in code by a function accepting an array of integers, each specifying the number of neurons in the layer associated with its index. Then we need to initialize the parameters . The parameters of a neural network need to be initialized before training. Turns out initializing them all to zero is not the right way. This is because the backpropagation algorithm can&#39;t work if all parameters have the same values. It is said that the parameters need to ‘break symmetry’. That is why here we will initialize weights to random small values and biases can be left at zero. Also, other problems may arise when initialization is not carefully handled but the details of parameter initialization techniques are outside the scope of this post. . Parameters are used in later steps so we store them in a dictionary . import numpy as np . . def initialize_neural_network(layer_dims): parameters = {} layer_count = len(layer_dims) for layer in range(1, layer_count): parameters[&#39;w&#39; + str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer - 1]) / np.sqrt(layer_dims[layer - 1]) parameters[&#39;b&#39; + str(layer)] = np.zeros((layer_dims[layer], 1)) return parameters . Forward Propagation . The forward propagation is synonymous with applying the neural network to a set of inputs that produces an output. This means we need to apply the layer functions. Each layer function is composed of two parts. First part is called the linear forward step ($z^{[l]}$) where each neuron calculates a weighted sum of all the inputs governed by our parameters. The second part is called the activation forward step ($a^{[l]}$) where a selected activation function ($g^{[l]}$) is applied to the output of the linear step and is sent to the next layer or the output. We can visualize these steps for a neural network by showing a computation graph. An example computation graph for a neural network with three layers ($L=3$) is shown below. . . Where: . $a^{[0]}$ is the input to the network, synonymous to X | $z^{[l]}$ is the output of a linear forward step for layer $l$ | $a^{[l]}$ is the output of an activation forward step for layer $l$ | $a^{[L]}$ is the output of the network | . Linear forward prop step is implemented as: . def linear_forward_prop_step(a_prev,w,b): z = np.dot(w, a_prev) + b return z . Now we need to implement the activation function. In as few words as possible, an activation function is used so the neural network can represent a non-linear representation. The default activation function used in hidden layers of neural networks is the ReLU function. ReLU is a simple function which maps a value to itself if it&#39;s positive, else it maps it to zero. ReLU is a good default choice because it has a stable gradient which is useful in the backpropagation algorithm. Since in this blog we are doing binary classification (is image an image of a cat), a default output layer activation function is the sigmoid function which maps values to a 0 to 1 range. The output will therefore correspond to how confident is the network that the image represents a cat. These activation functions are shown below. . . . Two mentioned activation functions are implemented as: . def sigmoid(z): a = 1/(1+np.exp(-z)) return a def relu(z): a=np.maximum(0,z) return a . We will let the user choose the activation by passing in its name in the activation forward prop step: . def activation_forward_prop_step(z,activation_name): if(activation_name==&#39;sigmoid&#39;): a=sigmoid(z) return a elif(activation_name==&#39;relu&#39;): a=relu(z) return a else: raise(&#39;Activation not supported&#39;) . Stepping through a single layer is than defined as: . def forward_prop_step(a_prev,w,b,activation_name): z=linear_forward_prop_step(a_prev,w,b) a=activation_forward_prop_step(z,activation_name) return z,a . Notice how we implemented everything modularly so we can easily generalize our calculations to different architectures as we will do right now. . Below is the implementation of the whole forward propagation to which we pass the input, parameters from the neural network initialization function and an array of activation names we use for each layer in turn. For backward propagation, mentioned later, we need to store all linear and activation outputs of layers in a cache. The input to the neural network can also be thought of as an activation so it is also stored as such. . def forward_prop(X, parameters, activation_names): layer_count = len(parameters) // 2 a = X forward_prop_cache = {} forward_prop_cache[&#39;a0&#39;]=X for layer in range(1, layer_count + 1): a_prev = a z, a = forward_prop_step(a_prev, parameters[&#39;w&#39; + str(layer)], parameters[&#39;b&#39; + str(layer)], activation_name=activation_names[layer - 1]) forward_prop_cache[&#39;z&#39; + str(layer)] = z forward_prop_cache[&#39;a&#39; + str(layer)] = a return forward_prop_cache . Cost calculation . A neural network produces some output for some input. If a neural network would be inferenced after parameter initialization the output would be gibberish because the neural network is not trained yet, but that&#39;s okay! To train our network with the backpropagation algorithm, we need a way to measure its performance. A cost function is used to measure performance by comparing the output/predictions of a neural network to the desired outputs shown in the dataset for a given sample. The cost function described here is the cross-entropy cost function. . $$J=- frac{1}{m} sum_{i=1}^{m} left(y^{(i)} log left(a^{[L](i)} right)+ left(1-y^{(i)} right) log left(1-a^{[L](i)} right) right)$$ . Cost calculation can be thought of as an additional block in the computation graph: . . def compute_cost(y_pred, y): m = y.shape[1] cost = (-1 / m) * np.sum(np.multiply(y, np.log(y_pred)) + np.multiply(1 - y, np.log(1 - y_pred))) cost = np.squeeze(cost) return cost . Backward Propagation . Backpropagation algorithm is an efficient method of computing gradients for the purpose of minimizing the cost function. In order to enable our neural network to ‘learn’ by minimizing the cost function, we need to alter our parameters. We can find out how we should alter our parameters by computing partial derivatives of the cost function with respect to each of our parameters. In other words, they will tell us approximately for each of our parameters how will the cost function behave if we increase or decrease them. Partial derivatives are calculated starting from the last layer to the first. This can be visualized with a computation graph. . . Calculating these partial derivatives includes applying the chain rule which yields the following expressions: . Partial derivatives for the parameters of the third layer with the respect to the cost function | . begin{aligned} frac{ partial C}{ partial w^{[3]}} &amp;= frac{ partial C}{ partial a^{[3]}} cdot frac{ partial a^{[3]}}{ partial z^{[3]}} cdot frac{ partial z^{[3]}}{ partial w^{[3]}} frac{ partial C}{ partial b^{[3]}} &amp;= frac{ partial C}{ partial a^{[3]}} cdot frac{ partial a^{[3]}}{ partial z^{[3]}} cdot frac{ partial z^{[3]}}{ partial b^{[3]}} end{aligned} Partial derivatives for the parameters of the second layer with the respect to the cost function | . begin{aligned} frac{ partial C}{ partial w^{[2]}} &amp;= frac{ partial C}{ partial a^{[3]}} cdot frac{ partial a^{[3]}}{ partial z^{[3]}} cdot frac{ partial z^{[3]}}{ partial a^{[2]}} cdot frac{ partial a^{[2]}}{ partial z^{[2]}} cdot frac{ partial z^{[2]}}{ partial w^{[2]}} frac{ partial C}{ partial b^{[2]}} &amp;= frac{ partial C}{ partial a^{[3]}} cdot frac{ partial a^{[3]}}{ partial z^{[3]}} cdot frac{ partial z^{[3]}}{ partial a^{[2]}} cdot frac{ partial a^{[2]}}{ partial z^{[2]}} cdot frac{ partial z^{[2]}}{ partial b^{[2]}} end{aligned} Partial derivatives for the parameters of the first layer with the respect to the cost function | . begin{aligned} frac{ partial C}{ partial w^{[1]}} &amp;= frac{ partial C}{ partial a^{[3]}} cdot frac{ partial a^{[3]}}{ partial z^{[3]}} cdot frac{ partial z^{[3]}}{ partial a^{[2]}} cdot frac{ partial a^{[2]}}{ partial z^{[2]}} cdot frac{ partial z^{[2]}}{ partial a^{[1]}} cdot frac{ partial a^{[1]}}{ partial z^{[1]}} cdot frac{ partial z^{[1]}}{ partial w^{[1]}} frac{ partial C}{ partial b^{[1]}} &amp;= frac{ partial C}{ partial a^{[3]}} cdot frac{ partial a^{[3]}}{ partial z^{[3]}} cdot frac{ partial z^{[3]}}{ partial a^{[2]}} cdot frac{ partial a^{[2]}}{ partial z^{[2]}} cdot frac{ partial z^{[2]}}{ partial a^{[1]}} cdot frac{ partial a^{[1]}}{ partial z^{[1]}} cdot frac{ partial z^{[1]}}{ partial b^{[1]}} end{aligned}In order to be able to efficiently calculate partial derivates for any neural network architecture we can precalculate a few repeating expressions. For details of how these equations came about, check Deriving And Implementing Backpropagation 2. . Note: Pay attention to how we chain these equations one into the next. . Partial derivative of the cost function with respect to the last activation forward prop step (output) $a^{[L]}$: | . $$ frac{ partial {C} }{ partial a^{[L]}} = - left( frac{y}{a^{[L]}}- frac{1-y}{1-a^{[L]}} right) $$ . def cost_backprop_step(y_pred,y,cost_name): if(cost_name==&#39;entropy&#39;): dC_da = - (np.divide(y, y_pred) - np.divide(1 - y, 1 - y_pred)) return dC_da . Partial derivative of the cost function with respect to the linear forward prop step is: | . $$ frac{ partial {C} }{ partial z^{[l]}} = frac{ partial {C} }{ partial a^{[l]}} g&#39;(z^{[l]}) $$ . where $g&#39;(z^{[l]})$ is the partial derivative of the activation function with respect to the input of the activation function $z^{[l]}$: | . $$ sigma^{ prime}(z^{[l]}) = sigma (z^{[l]}) (1- sigma(z^{[l]}))$$ . $$ ReLU&#39;(z^{[l]})= begin{cases}1, &amp; text{if} z^{[l]}&gt;0 0, &amp; text{otherwise} end{cases} $$ def activation_backward_prop_step(dC_da,z,activation_name): if(activation_name==&#39;sigmoid&#39;): dC_dz=dC_da*sigmoid_prime(z) assert (dC_dz.shape == z.shape) return dC_dz elif(activation_name==&#39;relu&#39;): dC_dz=dC_da*relu_prime(z) assert (dC_dz.shape == z.shape) return dC_dz . def sigmoid_prime(z): sigmoid = 1.0/(1.0+np.exp(-z)) return sigmoid * (1.0-sigmoid) def relu_prime(z): return np.where(z&gt;0, 1.0, 0.0) . Partial derivatives of our cost function with respect to parameters $w^{[l]}$, $b^{[l]}$ and the activation $a^{[l-1]}$: | . $$ frac{ partial {C} }{ partial w^{[l]}} = frac{1}{m} frac{ partial {C} }{ partial z^{[l]}} a^{[l-1] T} $$ . $$ frac{ partial {C} }{ partial b^{[l]}} = frac{1}{m} sum_{i = 1}^{m} frac{ partial {C} }{ partial z^{[l]}}^{(i)}$$ . $$ frac{ partial {C} }{ partial a^{[l-1]}} = {w^{[l]}}^{T} frac{ partial {C} }{ partial z^{[l]}} $$ . def linear_backward_prop_step(dC_dz,a_prev,w): m=a_prev.shape[1] dC_dw =(1/m)* np.dot(dC_dz, a_prev.T) dC_db = 1. / m * np.sum(dC_dz, axis=1, keepdims=True) dC_da_prev = np.dot(w.T, dC_dz) assert (dC_da_prev.shape == a_prev.shape) assert (dC_dw.shape == w.shape) return dC_da_prev, dC_dw, dC_db . Lets refactor the whole backward step through a single layer like: . def backward_prop_step(dC_da,a_prev,z,w,activation_name): dC_dz=activation_backward_prop_step(dC_da,z,activation_name) dC_da_prev,dC_dw,dC_db=linear_backward_prop_step(dC_dz,a_prev,w) return dC_da_prev, dC_dw, dC_db . Now we can describe the backpropagation of the whole network. . def backward_prop(y_pred, y, cost_name, parameters, forward_prop_cache, activation_names): backward_prop_cache = {} L = len(parameters) // 2 dC_daL = cost_backprop_step(y_pred, y, cost_name) backward_prop_cache[&#39;dC_da&#39; + str(L)] = dC_daL for l in reversed(range(1, L + 1)): dC_da = backward_prop_cache[&#39;dC_da&#39; + str(l)] w = parameters[&#39;w&#39; + str(l)] z = forward_prop_cache[&#39;z&#39; + str(l)] a_prev = forward_prop_cache[&#39;a&#39; + str(l-1)] dC_da_prev, dC_dw, dC_db = backward_prop_step(dC_da, a_prev, z, w, activation_names[l - 1]) backward_prop_cache[&#39;dC_dw&#39; + str(l)] = dC_dw backward_prop_cache[&#39;dC_db&#39; + str(l)] = dC_db if (l != 1): backward_prop_cache[&#39;dC_da&#39; + str(l - 1)] = dC_da_prev return backward_prop_cache . Parameter Update . After all gradients are found the parameters can be updated by altering their value in the direction that would lower the cost function. Since the gradient shows us how much the function is expected to increase, for a unit alteration of the variable, we need to subtract the gradient multiplied by something called the learning rate. Learning rate decides what fraction of the gradient we are applying to the parameters. . $$w^{[l]}=w^{[l]}- alpha frac{ partial C}{ partial w^{[l]}}$$ $$b^{[l]}=b^{[l]}- alpha frac{ partial C}{ partial b^{[l]}}$$ . def update_parameters(parameters, backward_prop_cache, learning_rate): L = len(parameters) // 2 for l in range(L): parameters[&quot;w&quot; + str(l + 1)] = parameters[&quot;w&quot; + str(l + 1)] - learning_rate * backward_prop_cache[ &quot;dC_dw&quot; + str(l + 1)] parameters[&quot;b&quot; + str(l + 1)] = parameters[&quot;b&quot; + str(l + 1)] - learning_rate * backward_prop_cache[ &quot;dC_db&quot; + str(l + 1)] return parameters . Applying a neural network . In this example a neural network is applied to solve a computer vision binary classification task. The task is recognizing cats in images. A dataset used consists out of 259 images with 105 of them being cat images and the rest 154 being non-cat images . Dataset preprocessing . . Warning: If you are following this in google colab, than you must download the dataset by expanding the next cell and following the instructions! . # If you are viewing this on google colab follow these instructions to acquire the dataset: # - If you don&#39;t have a kaggle account make one (its simple) # - Go to your profile -&gt; My account -&gt; Scroll to API -&gt; Create New API Token. kaggle.json file should be downloaded now to your computer # - Set the variable viewing_this_on_colab in this cell to True # - Execute the cell and when prompted upload your kaggle.json file viewing_this_on_colab=False import time import h5py import matplotlib.pyplot as plt import scipy from PIL import Image from scipy import ndimage if(viewing_this_on_colab): !pip install -q kaggle from google.colab import files files.upload() !mkdir ~/.kaggle/ !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json !kaggle datasets download -d muhammeddalkran/catvnoncat !unzip catvnoncat.zip train_catvnoncat_path=&#39;/content/catvnoncat/train_catvnoncat.h5&#39; test_catvnoncat_path=&#39;/content/catvnoncat/test_catvnoncat.h5&#39; else: train_catvnoncat_path=&#39;../tmp/2020-10-14/data/train_catvnoncat.h5&#39; test_catvnoncat_path=&#39;../tmp/2020-10-14/data/test_catvnoncat.h5&#39; . . train_dataset = h5py.File(train_catvnoncat_path,&quot;r&quot;) test_dataset = h5py.File(test_catvnoncat_path, &quot;r&quot;) train_x = np.array(train_dataset[&quot;train_set_x&quot;][:]) train_y = np.array(train_dataset[&quot;train_set_y&quot;][:]) train_y = train_y.reshape((1, train_y.shape[0])) print(&quot;train X shape:{} , train y shape:{}&quot;.format(train_x.shape,train_y.shape)) test_x= np.array(test_dataset[&quot;test_set_x&quot;][:]) test_y = np.array(test_dataset[&quot;test_set_y&quot;][:]) test_y = test_y.reshape((1, test_y.shape[0])) print(&quot;test X shape:{} , test y shape:{}&quot;.format(test_x.shape,test_y.shape)) classes = np.array(test_dataset[&quot;list_classes&quot;][:]) print(&#39;classes: {}&#39;.format(classes)) . train X shape:(209, 64, 64, 3) , train y shape:(1, 209) test X shape:(50, 64, 64, 3) , test y shape:(1, 50) classes: [b&#39;non-cat&#39; b&#39;cat&#39;] . An example of a cat and a non-cat image is shown below: . f=plt.figure() ax1=f.add_subplot(121) ax2=f.add_subplot(122) index=2 ax1.imshow(train_x[index]) index=5 ax2.imshow(train_x[index]) plt.show() . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-17T13:03:03.393852 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Now we flatten and normalize the pictures for the neural network . train_x_norm=train_x.reshape(train_x.shape[0],-1).T /255. test_x_norm=test_x.reshape(test_x.shape[0],-1).T /255. print(&#39;train_x_norm_shape: {}, test_x_norm_shape: {}&#39;.format(train_x_norm.shape,test_x_norm.shape)) . train_x_norm_shape: (12288, 209), test_x_norm_shape: (12288, 50) . Neural network training . When training a neural network the iteration count should be chosen. Iteration count determines how many times the neural network is going to evaluate itself and update its parameters. . np.random.seed(1) def NeuralNetworkTrain(X, y, layer_dims, learning_rate, num_iterations, activation_names, cost_name): costs = [] parameters = initialize_neural_network(layer_dims) layer_count=len(parameters)//2 for i in range(0, num_iterations): forward_prop_cache = forward_prop(X, parameters, activation_names) y_pred = forward_prop_cache[&#39;a&#39;+str(layer_count)] cost = compute_cost(y_pred, y) backward_prop_cache = backward_prop(y_pred, y, cost_name, parameters, forward_prop_cache, activation_names) parameters = update_parameters(parameters, backward_prop_cache, learning_rate) if i % 100 == 0: costs.append(cost) print(&#39;Iteration:{}, Cost={}&#39;.format(i, cost)) plt.plot(np.squeeze(costs)) plt.show() return parameters . Now lets choose our hyperparmeters and train our network! . activation_names=[&#39;relu&#39;,&#39;relu&#39;,&#39;relu&#39;,&#39;sigmoid&#39;] layer_dims=[12288,20,7,5,1] learning_rate=0.0075 num_iterations=3000 cost_name=&#39;entropy&#39; parameters=NeuralNetworkTrain(train_x_norm, train_y, layer_dims, learning_rate, num_iterations, activation_names, cost_name) . Iteration:0, Cost=0.7717493284237686 Iteration:100, Cost=0.6720534400822913 Iteration:200, Cost=0.6482632048575212 Iteration:300, Cost=0.6115068816101354 Iteration:400, Cost=0.5670473268366111 Iteration:500, Cost=0.54013766345478 Iteration:600, Cost=0.5279299569455268 Iteration:700, Cost=0.46547737717668525 Iteration:800, Cost=0.36912585249592816 Iteration:900, Cost=0.39174697434805333 Iteration:1000, Cost=0.3151869888600618 Iteration:1100, Cost=0.27269984417893894 Iteration:1200, Cost=0.23741853400268134 Iteration:1300, Cost=0.1996012053220864 Iteration:1400, Cost=0.1892630038846331 Iteration:1500, Cost=0.1611885466582775 Iteration:1600, Cost=0.1482138966236331 Iteration:1700, Cost=0.13777487812972944 Iteration:1800, Cost=0.12974017549190114 Iteration:1900, Cost=0.12122535068005202 Iteration:2000, Cost=0.11382060668633705 Iteration:2100, Cost=0.1078392852625413 Iteration:2200, Cost=0.10285466069352676 Iteration:2300, Cost=0.10089745445261789 Iteration:2400, Cost=0.09287821526472394 Iteration:2500, Cost=0.08841251177615041 Iteration:2600, Cost=0.08595130416146413 Iteration:2700, Cost=0.0816812691492633 Iteration:2800, Cost=0.07824661275815532 Iteration:2900, Cost=0.07544408693855478 . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-17T13:03:23.605060 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Result evaluation . The neural network is than evaluated on the training and test set by measuring accuracy, recall, precision and the F1 score . def print_results(X,y,parameters,activation_names,threshold=0.5): m = X.shape[1] n = len(parameters) // 2 p = np.zeros((1,m)) TP,TN,FP,FN=0,0,0,0 forward_prop_cache = forward_prop(X, parameters, activation_names) y_pred=forward_prop_cache[&#39;a&#39;+str(n)] for i in range(0, y_pred.shape[1]): if y_pred[0,i] &gt; threshold: p[0,i] = 1 if(y[0,i]==1): TP=TP+1 else: FP=FP+1 else: p[0,i] = 0 if(y[0,i]==0): TN=TN+1 else: FN=FN+1 print(&#39;question is: is it a cat?&#39;) print() print(&#39; ACTUALLY TRUE ACTUALLY FALSE&#39;) print(&#39;PREDICTED TRUE TP: {} FP: {}&#39;.format(TP,FP)) print(&#39;PREDICTED FALSE FN: {} TN: {}&#39;.format(FN,TN)) print() print(&#39;accuracy: {}&#39;.format((TP+TN)/(TP+TN+FP+FN))) print(&#39;precision: {}&#39;.format(TP/(TP+TN))) print(&#39;recall: {}&#39;.format(TP/(TP+FN))) print(&#39;f1: {}&#39;.format((TP / ( (TP+( (FN+FP) / 2) ))))) print() . print(&#39;--TRAINING SET RESULTS--&#39;) print_results(train_x_norm,train_y,parameters, activation_names=[&#39;relu&#39;,&#39;relu&#39;,&#39;relu&#39;,&#39;sigmoid&#39;]) print(&#39;--TEST SET RESULTS--&#39;) print_results(test_x_norm,test_y,parameters, activation_names=[&#39;relu&#39;,&#39;relu&#39;,&#39;relu&#39;,&#39;sigmoid&#39;]) . --TRAINING SET RESULTS-- question is: is it a cat? ACTUALLY TRUE ACTUALLY FALSE PREDICTED TRUE TP: 70 FP: 0 PREDICTED FALSE FN: 2 TN: 137 accuracy: 0.9904306220095693 precision: 0.33816425120772947 recall: 0.9722222222222222 f1: 0.9859154929577465 --TEST SET RESULTS-- question is: is it a cat? ACTUALLY TRUE ACTUALLY FALSE PREDICTED TRUE TP: 31 FP: 7 PREDICTED FALSE FN: 2 TN: 10 accuracy: 0.82 precision: 0.7560975609756098 recall: 0.9393939393939394 f1: 0.8732394366197183 . Lets predict the class of a random picture and show the result: . def predict(x,parameters,activation_names,threshold=0.5): n = len(parameters) // 2 forward_prop_cache = forward_prop(x, parameters, activation_names) y_pred=forward_prop_cache[&#39;a&#39;+str(n)] print(y_pred) . index=np.random.choice(range(50)) img=test_x[index,:] plt.imshow(img) . . &lt;matplotlib.image.AxesImage at 0x7fb683b27670&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-17T14:57:36.002707 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ img_flatten=test_x_norm[:,index] img_flatten=np.expand_dims(img_flatten, axis=1) print(&#39;Predicted percentage of it being a cat:&#39;) predict(img_flatten,parameters,activation_names) . . Predicted percentage of it being a cat: [[0.9999584]] . For additional material consider: . 1. Machine Learning Course by Stanford↩ . 2. Deriving and implementing backpropagation↩ .",
            "url": "https://bkoch4142.github.io/blog/jupyter/2020/10/14/Constructing-an-ANN.html",
            "relUrl": "/jupyter/2020/10/14/Constructing-an-ANN.html",
            "date": " • Oct 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Machine Learning Engineer with experience in ML, DL , NLP &amp; CV specializing in ConversationalAI &amp; NLP. .",
          "url": "https://bkoch4142.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bkoch4142.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}