{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - Intuition, Theory, Implementation\n",
    "> In this post we will be going over the intuition, theory and implementation of a gated RNN, particularly LSTM. This is the sequel to my previous post about RNNs.\n",
    "\n",
    "- toc: true\n",
    "- categories: [NLP]\n",
    "- hide: false\n",
    "- image: images/lstm.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Note: This post builds on my previous explanation of [RNNs](https://bkoch4142.github.io/blog/jupyter/nlp/2020/10/22/RNN-Intuition-Theory-Implementation.html). The dataset used in this post is the IMDB dataset of 50,000 movie reviews, used for sentiment classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RNN vs LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM, shorthand for Long-Short-Term-Memory, is a recurrent architecture for processing sequences just as the vanilla RNN. Compared to LSTM, the vanilla RNN is not used in practice as much, as it has some notable limitations that the LSTM architecture tries to address. These include:\n",
    "\n",
    "- Exploding/Vanishing Activations\n",
    "- Memory Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploding/Vanishing Activations**\n",
    "\n",
    "The problem of exploding/vanishing gradients occurs due to the way the vanilla RNN backpropagates gradients. If you consider the unrolled representation of an RNN and its backpropagation mechanism you will quickly notice that for long sequences there is a lot of repeated multiplication done in order for gradients to arrive from the last layer to the initial one. This is the exact reason why we might experience exploding/vanishing gradients. Let's explore this problem in more detail by simulating repeated multiplication in a backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let say we have a sequence of length 50. We will multiply matrices initialized with random numbers from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.randn(50,50)\n",
    "for i in range(50):\n",
    "    x= x @ torch.randn(50,50)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the occurence of exploding gradients. Basically, our numbers got so large that it caused numerical overflow and produced NaNs. What if we try to mitigate this by decreasing our matrices by a factor of 0.01?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.randn(50,50)\n",
    "for i in range(50):\n",
    "    x= x @ torch.randn(50,50)*0.01\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this produced vanishing gradients or numerical underflow. The numbers became so small that our computer just represents them with zeros. If we would have a weight matrix like this in any part of our neural network it would break the models learning capability, so that is why this is a delicate but important problem to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions to this include:\n",
    "- Identity initialization of weights paired with ReLU activation\n",
    "- Gradient clipping\n",
    "- Skip connections\n",
    "- Model alteration\n",
    "\n",
    "LSTM is an instance of a model approach to this problem. It mitigates the vanishing/exploding gradients problem by introducing the concept of gates which change the way gradients flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Memory Limitations**\n",
    "If you would experiment with RNNs, e.g. for next-word-prediction (language model), you would notice that they have a hard time with long-term memory needed in sentences such as this:\n",
    "\n",
    "_\"I grew up in France and had a wonderful childhood, therefore I speak fluent French\"_ -The language model would need to retain the information of \"France\" until its useful for predicting the word \"French\".\n",
    "\n",
    "LSTMs do a much better job with long-term memory because of several reasons:\n",
    "- An additional state called the cell state which enables accumulation of information over a long duration.\n",
    "- The concept of memory management depending on the current timestep's input and hidden state.\n",
    "\n",
    "#### **Cell State**\n",
    "LSTMs have an additional state called the cell state which is passed along with the hidden state to each cell. But the cell state has no linear layers through which it passes, therefore enabling easier information flow over a longer duration. This is what enables the long-term memory of an LSTM. Cell state is only influenced by element-wise operations controlled by gates which we will observe soon.\n",
    "\n",
    "#### **Memory Management**\n",
    "The term \"gated RNN\" comes from the fact that the cell state is gated (protected) by so-called gates. These gates are linear layers responsible for managing the cell state by extracting relevant information from the current timestep input and hidden state. The idea is that at each timestep the cell state information we don't need anymore should be forgotten and new valuable information should be stored. Since these gates are layers we delegate this mechanism for the neural network to learn itself without us having to manage it manually.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the LSTM Cell looks like: \n",
    "\n",
    "![\"Figure 1. - LSTM Cell Architecture\"](my_icons/lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four gates and each of them is fed the stacked tensor of the current timestep's hidden state and input. In summary, what each gate does for a sequence at a particular timestep is the following:\n",
    "\n",
    "- Forget gate: what information to keep and what to forget in the long memory\n",
    "- Input gate: what information needs to be updated in the long memory\n",
    "- Cell gate: how the information will be updated in the long memory\n",
    "- Output gate: what part of the long memory is relevant for the short memory\n",
    " \n",
    "\n",
    "### **Forget Gate**\n",
    "Forget gate decides what to forget/eliminate from the cell state. It is followed by a sigmoid function, mapping values to the (0, 1) range. We then multiply this output element-wise with the cell state. If the scalar at some position of the sigmoid output is closer to 0 it will result in the elimination of the value at the same position in the cell state. The opposite is true for values close to 1.\n",
    "\n",
    "### **Input Gate and Cell Gate**\n",
    "Input and Cell gate together decide what to update/store in the cell state.\n",
    "Input gate decides what needs to be updated and to what degree, dictated by the sigmoid. \n",
    "Cell gate decides what are the updated values for positions chosen by the input gate. Cell gate is followed by the tanh function mapping values to (-1, 1) range.\n",
    "The output of the input and cell gate is multiplied element-wise and then added element wise to the cell state.\n",
    "\n",
    "### **Output Gate**\n",
    "Output gate decides which information from the cell state is relevant for the next hidden state. Then this is fed to the sigmoid and the output is multiplied element-wise with the tanh of the updated cell state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementation**\n",
    "Below is a PyTorch implemenation of the LSTM Cell we described. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self,n_input,n_hidden):\n",
    "        super(LSTMCell,self).__init__()\n",
    "\n",
    "        self.forget_gate=nn.Linear(n_input+n_hidden, n_hidden)\n",
    "        self.input_gate=nn.Linear(n_input+n_hidden, n_hidden)\n",
    "        self.cell_gate=nn.Linear(n_input+n_hidden, n_hidden)\n",
    "        self.output_gate=nn.Linear(n_input+n_hidden, n_hidden)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "\n",
    "        h,c=state\n",
    "        forget_gate_out=torch.sigmoid(self.forget_gate(x))\n",
    "        input_gate_out=torch.sigmoid(self.input_gate(x))\n",
    "        cell_gate_out=torch.sigmoid(self.cell_gate(x))\n",
    "        output_gate_out=torch.sigmoid(self.output_gate(x))\n",
    "    \n",
    "        c=c*forget_gate_out\n",
    "        c=c+(input_gate_out*cell_gate_out)\n",
    "\n",
    "        out=output_gate_out*torch.tanh(c)\n",
    "\n",
    "        return out,(h,c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: This particular implementation is not efficient, its only a toy example. For an efficient implementation merge the 4 matrix multiplications into 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, all parts of an LSTM are now clear. For an even better understanding continue to see it applied to sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be testing LSTM's performance on the IMDB dataset for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "619    The quote above just about says it all for \"Sl...  negative\n",
      "33422  normally,i would say i loved this movie.not fo...  negative\n",
      "28228  When i heard about this movie it was supposed ...  positive\n",
      "28504  (You'll know what I mean after you've seen Red...  positive\n",
      "8085   I enjoyed Still Crazy more than any film I hav...  positive\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "data=pd.read_csv('./imdb.csv')\n",
    "data=data.sample(frac=1)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "train=data[:25000]\n",
    "xtrain=train['review'].values.tolist()\n",
    "ytrain=train['sentiment'].values\n",
    "\n",
    "val=data[25000:]\n",
    "xval=val['review'].values.tolist()\n",
    "yval=val['sentiment'].values\n",
    "\n",
    "# Preprocessing (Tokenization and padding)\n",
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=3000)\n",
    "tokenizer.fit_on_texts(xtrain)\n",
    "\n",
    "xtrain_pro=tokenizer.texts_to_sequences(xtrain)\n",
    "xtrain_pro=tf.keras.preprocessing.sequence.pad_sequences(xtrain_pro, maxlen=128)\n",
    "\n",
    "xval_pro=tokenizer.texts_to_sequences(xval)\n",
    "xval_pro=tf.keras.preprocessing.sequence.pad_sequences(xval_pro, maxlen=128)\n",
    "\n",
    "ytrain=[1 if y=='positive' else 0 for y in ytrain]\n",
    "yval=[1 if y=='positive' else 0 for y in yval]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our dataset class and our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset:\n",
    "    def __init__(self,reviews,targets):\n",
    "        self.reviews=reviews\n",
    "        self.targets=targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        review=self.reviews[idx,:]\n",
    "        target=self.targets[idx]\n",
    "\n",
    "        review=torch.tensor(review,dtype=torch.long)\n",
    "        target=torch.tensor(target,dtype=torch.float)\n",
    "\n",
    "        item=(review,\n",
    "              target)\n",
    "\n",
    "        return item\n",
    "    \n",
    "train_ds=IMDBDataset(xtrain_pro, ytrain)\n",
    "val_ds=IMDBDataset(xval_pro, yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create our dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl=torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=128,\n",
    "        )\n",
    "\n",
    "val_dl=torch.utils.data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=128,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the SentimentClassifier Model with our LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply the LSTMCell we package it in the LSTM class which applies it to a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,n_input,n_hidden):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.n_input=n_input\n",
    "        self.n_hidden=n_hidden\n",
    "\n",
    "        self.LSTMCell=LSTMCell(n_input,n_hidden)\n",
    "\n",
    "    def forward(self,input,state=None):\n",
    "        \n",
    "        # Input dims are (batch_size, seq_length, timestep_features)\n",
    "        sequence_length=input.size()[1]\n",
    "\n",
    "        # Initialize hidden state to zeros if not provided\n",
    "        if state==None:\n",
    "            state=(torch.zeros((input.size()[0],self.n_hidden),device=input.device),\n",
    "                    torch.zeros((input.size()[0],self.n_hidden),device=input.device))\n",
    "        h,c=state\n",
    "\n",
    "        outs=torch.tensor([],device=input.device)\n",
    "        for i in range(sequence_length):\n",
    "            x_timestep_features=torch.squeeze(input[:,i,:],dim=1)\n",
    "            x_timestep_features=torch.cat([h, x_timestep_features], dim=1)\n",
    "\n",
    "            out, (h,c)=self.LSTMCell(x_timestep_features, (h,c))\n",
    "\n",
    "            out=torch.unsqueeze(out,dim=1)\n",
    "            outs=torch.cat((outs,out), dim=1)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we package our LSTM into a SentimentClassifier class which uses an additional embedding layer and linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self,vocab_sz,n_hidden):\n",
    "        super(SentimentClassifier,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_sz,n_hidden)\n",
    "        self.rnn=LSTM(n_hidden,n_hidden)\n",
    "        self.linear=nn.Linear(n_hidden*2,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.embedding(x)\n",
    "        x =self.rnn(x)\n",
    "\n",
    "        # Using the avg and max pool of all RNN outputs\n",
    "        avg_pool=torch.mean(x, dim=1)\n",
    "        max_pool, _ =torch.max(x,1)\n",
    "\n",
    "        # We concatenate them (hidden size before the linear layer is multiplied by 2)\n",
    "        out=torch.cat((avg_pool, max_pool), dim=1)\n",
    "        out=self.linear(out)\n",
    "\n",
    "        return torch.squeeze(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SentimentClassifier(3000,100)\n",
    "model=model.to(device)\n",
    "loss_func=torch.nn.BCEWithLogitsLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "sigmoid= lambda x: 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training accuracy: 0.70\n",
      "Epoch: 0 | Validation accuracy: 0.77\n",
      "----------------------------------------\n",
      "Epoch: 1 | Training accuracy: 0.81\n",
      "Epoch: 1 | Validation accuracy: 0.83\n",
      "----------------------------------------\n",
      "Epoch: 2 | Training accuracy: 0.85\n",
      "Epoch: 2 | Validation accuracy: 0.85\n",
      "----------------------------------------\n",
      "Epoch: 3 | Training accuracy: 0.88\n",
      "Epoch: 3 | Validation accuracy: 0.86\n",
      "----------------------------------------\n",
      "Epoch: 4 | Training accuracy: 0.89\n",
      "Epoch: 4 | Validation accuracy: 0.87\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    # Training\n",
    "    ys=[]\n",
    "    y_preds=[]\n",
    "    for x,y in train_dl:\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        \n",
    "        y_pred=model(x)\n",
    "        loss=loss_func(y_pred,y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ys.extend(y.detach().cpu().numpy().tolist())\n",
    "        y_preds.extend(y_pred.detach().cpu().numpy().tolist())\n",
    "        \n",
    "    # Measuring Training accuracy\n",
    "    y_preds_final=sigmoid(np.array(y_preds))>0.5\n",
    "    accuracy=accuracy_score(y_preds_final,ys)\n",
    "    print(f\"Epoch: {epoch} | Training accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    # Validation\n",
    "    ys=[]\n",
    "    y_preds=[]\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_dl:\n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            \n",
    "            y_pred=model(x)\n",
    "\n",
    "            ys.extend(y.detach().cpu().numpy().tolist())\n",
    "            y_preds.extend(y_pred.detach().cpu().numpy().tolist())\n",
    "    \n",
    "    # Measuring Validation accuracy\n",
    "    y_preds_final=sigmoid(np.array(y_preds))>0.5\n",
    "    accuracy=accuracy_score(y_preds_final,ys)\n",
    "    print(f\"Epoch: {epoch} | Validation accuracy: {accuracy:.2f}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the end we can test our model on arbitrary input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide \n",
    "\n",
    "def predict(model,text):\n",
    "    x=tokenizer.texts_to_sequences(text)\n",
    "    x=tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=128)\n",
    "    x=torch.tensor(x,device='cuda')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logit=model(x)\n",
    "        prob=sigmoid(logit.cpu().numpy())\n",
    "        \n",
    "    print(f\"Output: {str(prob)} | \", end='')\n",
    "    \n",
    "    if prob>=0.5:\n",
    "        print(f'Sentiment: positive')\n",
    "    else:\n",
    "        print(f'Sentiment: negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0.13759616] | Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "text=['This guy made this blog about LSTMs and provided the implementation without explaining each line of the code!']\n",
    "predict(model,text)"
   ]
  },
  {
   "source": [
    "## Thanks for Reading!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}